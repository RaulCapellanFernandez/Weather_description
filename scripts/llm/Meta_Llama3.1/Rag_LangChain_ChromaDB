{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vXimiczfRdci"},"outputs":[],"source":["# Set the Google Drive folder\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/Github/Weather_description/\n","\n","# Install the necesary dependencies\n","! pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gf0XLDj07bMt"},"outputs":[],"source":["# Import the libraries\n","import os\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","from dotenv import load_dotenv\n","\n","load_dotenv()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ly4G21St_T6T"},"outputs":[],"source":["# Download the model from Hugging Face\n","model_id   = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","hf_user    = os.getenv('HUGGINGFACE_USER')\n","hf_token   = os.getenv('HUGGINGFACE_TOKEN_READ')\n","path_model = './resources/llama3.1'\n","\n","if os.path.isdir(path_model) == False:\n","  ! git lfs install\n","  ! git lfs pull\n","  ! git clone https://{hf_user}:{hf_token}@huggingface.co/{model_id} {path_model}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLCrsEhw-DA5"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(path_model)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    path_model,\n","    device_map=\"auto\",\n","    torch_dtype=\"auto\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15hnf-unOgk1"},"outputs":[],"source":["# Inference oof the model in local\n","messages = [\n","    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n","    {\"role\": \"user\", \"content\": \"Who are you?\"},\n","]\n","prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","# Tokenize and move to model device\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","# Generate response\n","outputs = model.generate(**inputs, max_new_tokens=150, do_sample=True)\n","\n","# Decode the output\n","response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","print(\"Model response:\")\n","print(response)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNb0NEad3dmO3rB+0VXGgeO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}