{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e31c8f8-e113-4ca7-8948-c540ac4d6d22",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------\n",
    "## Base Model\n",
    "- Using CNN and RNN in a mixing way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "791acc2e-05da-435f-b5ec-dc69cd9fd2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.image import load_img , img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input\n",
    "from tensorflow.keras.applications import ResNet152\n",
    "from tensorflow.keras.applications.densenet import DenseNet201\n",
    "from tensorflow.keras.layers import Input , Dense , LSTM , Embedding , Dropout , add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86432cab-9e3e-49fa-a9a5-ebc28e924efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_path = os.path.join('..', '..', 'resources')\n",
    "path_openAI    = os.path.join(resources_path, 'utils', 'images_weather_description_openAI.csv')\n",
    "path_ibericam  = os.path.join(resources_path, 'images_ibericam')\n",
    "path_captions  = os.path.join(resources_path, 'captions.txt')\n",
    "path_model     = os.path.join(resources_path, 'utils', 'best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9915dc-5763-413d-975e-6d55bfa5f0e4",
   "metadata": {},
   "source": [
    "### 1. Generate the mapping for captions in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e6c638-3a88-4f0e-9f89-bbd64da6117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_openAI   = pd.read_csv(path_openAI)\n",
    "list_images_ibercam = glob(os.path.join(path_ibericam, '*.jpg'))\n",
    "\n",
    "# Delete this lines for working with the hole dataset\n",
    "df_openAI = df_openAI[df_openAI.path_image.isin(list_images_ibercam)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4618684-80c0-4301-852a-b75f26248ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux               = df_openAI[['path_image', 'description']]\n",
    "df_aux               = df_aux.copy()  # Create a full copy to avoid the warning\n",
    "df_aux['path_image'] = df_aux['path_image'].apply(lambda x: x.split(os.sep)[-1])\n",
    "df_aux.columns       = ['image', 'caption']\n",
    "\n",
    "# Get the name of the images\n",
    "df_aux['image_name'] = df_aux[\"image\"].str.split('.').str[0]\n",
    "\n",
    "# Cleaning and formating the dataset before applying LSTM\n",
    "df_aux['caption_mod'] = df_aux[\"caption\"].apply(lambda caption: [f'startseq \"{re.sub(\"[^A-Za-z . -]\", \"\", caption.lower())}\" endseq'])\n",
    "\n",
    "# Creating a dictinary with the name of the photos and the captions\n",
    "mapping_dic          = dict(zip(df_aux['image_name'], df_aux['caption_mod']))\n",
    "list_captions        = df_aux['caption_mod'].values\n",
    "list_captions        = [item[0] for item in list_captions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d804fc4-0ff2-4c94-abfa-7b527a4559bc",
   "metadata": {},
   "source": [
    "### 2. Load CNN prebuild models\n",
    "- Modify the preloaded models from keras to:\n",
    "    - Inputs: mantains the original inputs of the original model\n",
    "    - Outputs: Takes the second last layer of the model avoiding the last one to be used later as an input to the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df26e6bd-2eb1-4759-9f98-6e744a0591d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 16 layer network using 3x3 convolutions image classification #####\n",
    "model = VGG16()\n",
    "##### Dense networks with direct connections among all the layers to improve the gradient flow #####\n",
    "# model = DenseNet201()\n",
    "##### Residual networks, prevents performance degradation #####\n",
    "# model = ResNet152()\n",
    "\n",
    "\n",
    "# model = Model(inputs = model.inputs , outputs = model.layers[-2].output)\n",
    "\n",
    "desired_output = model.get_layer('fc2').output\n",
    "model = Model(inputs=model.input, outputs=desired_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d4258-c79d-4e1b-ac6f-6427529f3e39",
   "metadata": {},
   "source": [
    "### 3. Transform the images\n",
    "- Open the images and adapt it to work with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "439f0b50-90e8-4897-8768-b0feb3d05599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ba3bc9f804441aae70d7ef240e7e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rcapella\\anaconda3\\envs\\tf_env\\Lib\\site-packages\\keras\\src\\models\\functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor']\n",
      "Received: inputs=Tensor(shape=(1, 224, 224, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "list_images_ibercam = glob(os.path.join(path_ibericam, '*.jpg'))\n",
    "\n",
    "for index, img_path in enumerate(tqdm(list_images_ibercam)):\n",
    "    \n",
    "    # load the image from file in a certain format adapted to the model size\n",
    "    image = load_img(img_path, target_size= (224, 224))#(model.input.shape[1], model.input.shape[2]))\n",
    "    \n",
    "    # convert image pixels to numpy array\n",
    "    image = img_to_array(image)\n",
    "    \n",
    "    # reshape data for model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    \n",
    "    # preprocess image for tensorflow\n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    # extract features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "\n",
    "    \n",
    "    # Get image name\n",
    "    image_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    \n",
    "    # store feature\n",
    "    features[image_id] = feature\n",
    "\n",
    "# Store the features of the images in a pickle file (for saving time for running multiple experiments)\n",
    "pickle.dump(features, open(os.path.join(resources_path, 'features.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc005e-e757-4b33-b7ca-0633785763e4",
   "metadata": {},
   "source": [
    "### 6. Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018c41a1-2f07-4c03-82d5-c9a6ce992d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "\n",
    "    while True:\n",
    "        for key in data_keys:\n",
    "            captions = mapping[key]\n",
    "            for caption in captions:\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                if len(seq) < 2:\n",
    "                    continue\n",
    "\n",
    "                in_seq = seq[:-1]\n",
    "                out_seq = seq[1:]\n",
    "\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n",
    "                out_seq = pad_sequences([out_seq], maxlen=max_length, padding='post')[0]\n",
    "\n",
    "                out_seq = to_categorical(out_seq, num_classes=vocab_size)\n",
    "\n",
    "                X1.append(features[key].reshape(-1))  # asegura dimensión correcta\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "\n",
    "                n += 1\n",
    "                if n == batch_size:\n",
    "                    yield ((np.array(X1), np.array(X2)), np.array(y))\n",
    "                    X1, X2, y = list(), list(), list()\n",
    "                    n = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93fe8e-6ed1-471a-982a-aba7ac3fb6ec",
   "metadata": {},
   "source": [
    "### 4. Tokenizer the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd4cba5b-03bf-4e0b-8561-63c8297e0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Maximum size of one of the captions\n",
    "max_length = max(len(caption.split()) for caption in list_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1a61d-3e6e-44c2-84f8-b44e689b33a9",
   "metadata": {},
   "source": [
    "### 5. Create LSTM model\n",
    "- **shape=(4096,)** - output length of the features from the VGG model\n",
    "\n",
    "- **Dense** - single dimension linear layer array\n",
    "\n",
    "- **Dropout()** - used to add regularization to the data, avoiding over fitting & dropping out a fraction of the data from the layers\n",
    "\n",
    "- **model.compile()** - compilation of the model\n",
    "\n",
    "- **loss=’sparse_categorical_crossentropy’** - loss function for category outputs\n",
    "\n",
    "- **optimizer=’adam’** - automatically adjust the learning rate for the model over the no. of epochs\n",
    "\n",
    "- Model plot shows the concatenation of the inputs and outputs into a single layer\n",
    "\n",
    "- Feature extraction of image was already done using VGG, no CNN model was needed in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed0f708a-f952-4d82-ac97-73ea24118537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ sequence_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">100,864</span> │ sequence_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048,832</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
       "│                               │                           │                 │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],             │\n",
       "│                               │                           │                 │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">394</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">101,258</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image_input (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ sequence_input (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m88\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m88\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m100,864\u001b[0m │ sequence_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │       \u001b[38;5;34m1,048,832\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m88\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │          \u001b[38;5;34m65,792\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │          \u001b[38;5;34m65,792\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m88\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m525,312\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
       "│                               │                           │                 │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],             │\n",
       "│                               │                           │                 │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m88\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │          \u001b[38;5;34m65,792\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m88\u001b[0m, \u001b[38;5;34m394\u001b[0m)           │         \u001b[38;5;34m101,258\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,973,642</span> (7.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,973,642\u001b[0m (7.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,973,642</span> (7.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,973,642\u001b[0m (7.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Entrada de características de la imagen\n",
    "inputs_img = Input(shape=(4096,), name='image_input')\n",
    "img_features = Dropout(0.4)(inputs_img)\n",
    "img_features = Dense(256, activation='relu')(img_features)\n",
    "\n",
    "# Entrada de secuencia de texto\n",
    "inputs_seq = Input(shape=(max_length,), name='sequence_input')\n",
    "seq_embedding = Embedding(vocab_size, 256, mask_zero=True)(inputs_seq)\n",
    "seq_embedding = Dropout(0.4)(seq_embedding)\n",
    "\n",
    "# Inicializar estados del LSTM a partir de características de la imagen\n",
    "state_h = Dense(256, activation='relu')(img_features)\n",
    "state_c = Dense(256, activation='relu')(img_features)\n",
    "\n",
    "# Capa LSTM usando las características de imagen como estado inicial\n",
    "lstm_out = LSTM(256, return_sequences=True)(seq_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "# Capas finales de clasificación\n",
    "decoder_dense = Dense(256, activation='relu')(lstm_out)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder_dense)\n",
    "\n",
    "# Definición del modelo\n",
    "model = Model(inputs=[inputs_img, inputs_seq], outputs=outputs)\n",
    "\n",
    "# Compilación\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443c99b-406c-4bbe-ae27-8ad7d4b2dfd3",
   "metadata": {},
   "source": [
    "### 5. Prepare data for trainning\n",
    "- Train split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65e21491-25d3-49f8-a077-52448e5e3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = list(mapping_dic.keys())\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe55387-ec74-4d5f-83e0-4ecb91e9b7aa",
   "metadata": {},
   "source": [
    "### 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8103ff38-4892-4c0d-bc2b-ab924f2dd16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 316ms/step - accuracy: 0.0309 - loss: 5.7965\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 369ms/step - accuracy: 0.0405 - loss: 4.7075\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 303ms/step - accuracy: 0.0518 - loss: 4.4585\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 303ms/step - accuracy: 0.0645 - loss: 4.3359\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 337ms/step - accuracy: 0.0967 - loss: 4.2048\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 302ms/step - accuracy: 0.1228 - loss: 3.9960\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 302ms/step - accuracy: 0.1596 - loss: 3.6992\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 326ms/step - accuracy: 0.1978 - loss: 3.3823\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 314ms/step - accuracy: 0.2292 - loss: 3.0827\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 324ms/step - accuracy: 0.2555 - loss: 2.8181\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 317ms/step - accuracy: 0.2724 - loss: 2.5900\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 289ms/step - accuracy: 0.2983 - loss: 2.3938\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 314ms/step - accuracy: 0.3175 - loss: 2.2275\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 330ms/step - accuracy: 0.3303 - loss: 2.0945\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 330ms/step - accuracy: 0.3426 - loss: 1.9774\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 337ms/step - accuracy: 0.3484 - loss: 1.8847\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 299ms/step - accuracy: 0.3602 - loss: 1.8108\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 299ms/step - accuracy: 0.3662 - loss: 1.7538\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 316ms/step - accuracy: 0.3709 - loss: 1.7031\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 288ms/step - accuracy: 0.3704 - loss: 1.6559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "steps = len(train) // batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    # create data generator\n",
    "    generator = data_generator(train, mapping_dic, features, tokenizer, max_length, vocab_size, batch_size)\n",
    "    # fit for one epoch\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\n",
    "# Save the best model\n",
    "model.save(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f3c1e-be7c-4ac8-bc1a-23a3f45c1abb",
   "metadata": {},
   "source": [
    "### 8. Generate Captions for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "361a305e-1227-4c3b-9e8a-35f921c5b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "456395a4-8d4f-4101-9354-5f46cb91bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Encode and pad the input sequence\n",
    "        sequence = pad_sequences([tokenizer.texts_to_sequences([in_text])[0]], maxlen=max_length)\n",
    "        # Predict the next word\n",
    "        yhat = np.argmax(model.predict([image, sequence], verbose=0))\n",
    "        # Convert the predicted index to a word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        \n",
    "        if not word or word == 'endseq':  # Stop if word not found or end tag is reached\n",
    "            break\n",
    "        \n",
    "        in_text += f\" {word}\"  # Append the word to the input sequence\n",
    "    \n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bac3a6-e7a4-4662-91aa-07f798b36100",
   "metadata": {},
   "source": [
    "### 9. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28516e5-c3bc-4c59-8e58-dc4ce11405d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea6b8f8052a4cde9ae82096014a8257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in tqdm(test):\n",
    "    # get actual caption\n",
    "    captions = mapping_dic[key]\n",
    "    # predict the caption for image\n",
    "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
    "    # split into words\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    # append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "# calcuate BLEU score\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb44eb-65d1-4190-9595-f027508e4536",
   "metadata": {},
   "source": [
    "## 10. Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a5a8e-197a-4b85-91b8-68a3adbddde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(img_path):\n",
    "    image = Image.open(img_path)\n",
    "    captions = mapping_dic[image_id]\n",
    "    print('---------------------Actual---------------------')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    # predict the caption\n",
    "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
    "    print('--------------------Predicted--------------------')\n",
    "    print(y_pred)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16bf9a8-8e38-4607-9076-ffe08320ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(\"../../resources/images_ibericam/albentosa-20230429-130501.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
