{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e31c8f8-e113-4ca7-8948-c540ac4d6d22",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------\n",
    "## Convolutional + attention model\n",
    "- Using CNN and Trasnformers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791acc2e-05da-435f-b5ec-dc69cd9fd2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.image import load_img , img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input\n",
    "from tensorflow.keras.applications import ResNet152\n",
    "from tensorflow.keras.applications.densenet import DenseNet201\n",
    "from tensorflow.keras.layers import Input , Dense , LSTM , Embedding , Dropout , add\n",
    "\n",
    "from tensorflow.keras.layers import Add, Activation, RepeatVector, Concatenate\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86432cab-9e3e-49fa-a9a5-ebc28e924efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_path = os.path.join('..', '..', 'resources')\n",
    "path_openAI    = os.path.join(resources_path, 'utils', 'images_weather_description_openAI.csv')\n",
    "path_ibericam  = os.path.join(resources_path, 'images_ibericam')\n",
    "path_captions  = os.path.join(resources_path, 'captions.txt')\n",
    "path_model     = os.path.join(resources_path, 'utils', 'best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9915dc-5763-413d-975e-6d55bfa5f0e4",
   "metadata": {},
   "source": [
    "### 1. Generate the mapping for captions in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6c638-3a88-4f0e-9f89-bbd64da6117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_openAI   = pd.read_csv(path_openAI)\n",
    "list_images_ibercam = glob(os.path.join(path_ibericam, '*.jpg'))\n",
    "\n",
    "# Delete this lines for working with the hole dataset\n",
    "df_openAI = df_openAI[df_openAI.path_image.isin(list_images_ibercam)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4618684-80c0-4301-852a-b75f26248ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux               = df_openAI[['path_image', 'description']]\n",
    "df_aux               = df_aux.copy()  # Create a full copy to avoid the warning\n",
    "df_aux['path_image'] = df_aux['path_image'].apply(lambda x: x.split(os.sep)[-1])\n",
    "df_aux.columns       = ['image', 'caption']\n",
    "\n",
    "# Get the name of the images\n",
    "df_aux['image_name'] = df_aux[\"image\"].str.split('.').str[0]\n",
    "\n",
    "# Cleaning and formating the dataset before applying LSTM\n",
    "df_aux['caption_mod'] = df_aux[\"caption\"].apply(lambda caption: [f'startseq \"{re.sub(\"[^A-Za-z . -]\", \"\", caption.lower())}\" endseq'])\n",
    "\n",
    "# Creating a dictinary with the name of the photos and the captions\n",
    "mapping_dic          = dict(zip(df_aux['image_name'], df_aux['caption_mod']))\n",
    "list_captions        = df_aux['caption_mod'].values\n",
    "list_captions        = [item[0] for item in list_captions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d804fc4-0ff2-4c94-abfa-7b527a4559bc",
   "metadata": {},
   "source": [
    "### 2. Load CNN prebuild models\n",
    "- Modify the preloaded models from keras to:\n",
    "    - Inputs: mantains the original inputs of the original model\n",
    "    - Outputs: Takes the second last layer of the model avoiding the last one to be used later as an input to the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26e6bd-2eb1-4759-9f98-6e744a0591d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 16 layer network using 3x3 convolutions image classification #####\n",
    "model = VGG16()\n",
    "##### Dense networks with direct connections among all the layers to improve the gradient flow #####\n",
    "# model = DenseNet201()\n",
    "##### Residual networks, prevents performance degradation #####\n",
    "# model = ResNet152()\n",
    "\n",
    "model = Model(inputs = model.inputs , outputs = model.layers[-2].output)\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d4258-c79d-4e1b-ac6f-6427529f3e39",
   "metadata": {},
   "source": [
    "### 3. Transform the images\n",
    "- Open the images and adapt it to work with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f0b50-90e8-4897-8768-b0feb3d05599",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "for img_path in tqdm(list_images_ibercam):\n",
    "    # load the image from file in a certain format adapted to the model size\n",
    "    image = load_img(img_path, target_size=(224, 224))\n",
    "    \n",
    "    # convert image pixels to numpy array\n",
    "    image = img_to_array(image)\n",
    "    \n",
    "    # expand dimensions to add batch size dimension\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    # preprocess image for TensorFlow\n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    # extract features (model expects numpy array with batch dimension)\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    \n",
    "    # Get image name\n",
    "    image_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    \n",
    "    # Store feature\n",
    "    features[image_id] = feature.squeeze()  # Remove extra dimensions if present\n",
    "\n",
    "# Save features to pickle\n",
    "pickle.dump(features, open(os.path.join(resources_path, 'features.pkl'), 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc005e-e757-4b33-b7ca-0633785763e4",
   "metadata": {},
   "source": [
    "### 6. Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c41a1-2f07-4c03-82d5-c9a6ce992d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    # loop over images\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    \n",
    "    while True:\n",
    "        for key in data_keys:\n",
    "            n += 1\n",
    "            captions = mapping[key]\n",
    "            # print(captions)\n",
    "            # process each caption\n",
    "            for caption in captions:\n",
    "                # encode the sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                # split the sequence into X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pairs\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\n",
    "                    # store the sequences\n",
    "                    # X1.append(features[key][0])\n",
    "                    X1.append(features[key])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n == batch_size:\n",
    "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                yield (X1, X2), y\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31565784-204d-493e-baa3-3ddc35d618f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "\n",
    "    while True:\n",
    "        for key in data_keys:\n",
    "            captions = mapping[key]\n",
    "            for caption in captions:\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                # Skip captions shorter than 2 tokens\n",
    "                if len(seq) < 2:\n",
    "                    continue\n",
    "\n",
    "                # Create input-output pairs for the full sequence\n",
    "                in_seq = seq[:-1]\n",
    "                out_seq = seq[1:]\n",
    "\n",
    "                # Pad input and output sequences to max_length\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n",
    "                out_seq = pad_sequences([out_seq], maxlen=max_length, padding='post')[0]\n",
    "\n",
    "                # Convert output sequence to categorical format (max_length, vocab_size)\n",
    "                out_seq = to_categorical(out_seq, num_classes=vocab_size)\n",
    "\n",
    "                X1.append(features[key])  # image features\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "\n",
    "                n += 1\n",
    "                if n == batch_size:\n",
    "                    yield (np.array(X1), np.array(X2)), np.array(y)\n",
    "                    X1, X2, y = list(), list(), list()\n",
    "                    n = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93fe8e-6ed1-471a-982a-aba7ac3fb6ec",
   "metadata": {},
   "source": [
    "### 4. Tokenizer the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4cba5b-03bf-4e0b-8561-63c8297e0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Maximum size of one of the captions\n",
    "max_length = max(len(caption.split()) for caption in list_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1a61d-3e6e-44c2-84f8-b44e689b33a9",
   "metadata": {},
   "source": [
    "### 5. Create LSTM model\n",
    "- **shape=(4096,)** - output length of the features from the VGG model\n",
    "\n",
    "- **Dense** - single dimension linear layer array\n",
    "\n",
    "- **Dropout()** - used to add regularization to the data, avoiding over fitting & dropping out a fraction of the data from the layers\n",
    "\n",
    "- **model.compile()** - compilation of the model\n",
    "\n",
    "- **loss=’sparse_categorical_crossentropy’** - loss function for category outputs\n",
    "\n",
    "- **optimizer=’adam’** - automatically adjust the learning rate for the model over the no. of epochs\n",
    "\n",
    "- Model plot shows the concatenation of the inputs and outputs into a single layer\n",
    "\n",
    "- Feature extraction of image was already done using VGG, no CNN model was needed in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f708a-f952-4d82-ac97-73ea24118537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (you can adjust these)\n",
    "max_length = 50\n",
    "vocab_size = 5000\n",
    "embed_size = 256\n",
    "num_heads = 4  # number of attention heads\n",
    "\n",
    "# CNN Feature Input\n",
    "inputs1 = Input(shape=(4096,))  # Assuming you already have precomputed CNN features\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(embed_size, activation='relu')(fe1)  # Image embedding [batch_size, embed_size]\n",
    "\n",
    "# Expand image features for attention mechanism\n",
    "fe2_expanded = RepeatVector(max_length)(fe2)  # shape: (batch_size, max_length, embed_size)\n",
    "\n",
    "# Sequence Input\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, embed_size, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)  # shape: (batch_size, max_length, embed_size)\n",
    "\n",
    "# Attention Layer (Multi-head Attention)\n",
    "attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=embed_size)(query=se2, value=fe2_expanded, key=fe2_expanded)\n",
    "\n",
    "# Combine Attention Output with Embedding (residual connection)\n",
    "decoder_input = Add()([attention_out, se2])\n",
    "decoder_input = LayerNormalization()(decoder_input)\n",
    "\n",
    "# Feedforward Layer (can be expanded if desired)\n",
    "decoder_ff = Dense(embed_size, activation='relu')(decoder_input)\n",
    "\n",
    "# Generate caption output (per timestep prediction)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder_ff)\n",
    "\n",
    "# Final CNN-Attention Model\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443c99b-406c-4bbe-ae27-8ad7d4b2dfd3",
   "metadata": {},
   "source": [
    "### 5. Prepare data for trainning\n",
    "- Train split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e21491-25d3-49f8-a077-52448e5e3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = list(mapping_dic.keys())\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "steps = len(train) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe55387-ec74-4d5f-83e0-4ecb91e9b7aa",
   "metadata": {},
   "source": [
    "### 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103ff38-4892-4c0d-bc2b-ab924f2dd16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    # create data generator\n",
    "    generator = data_generator(train, mapping_dic, features, tokenizer, max_length, vocab_size, batch_size)\n",
    "    # fit for one epoch\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\n",
    "# Save the best model\n",
    "model.save(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f3c1e-be7c-4ac8-bc1a-23a3f45c1abb",
   "metadata": {},
   "source": [
    "### 8. Generate Captions for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a305e-1227-4c3b-9e8a-35f921c5b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456395a4-8d4f-4101-9354-5f46cb91bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Encode and pad the input sequence\n",
    "        sequence = pad_sequences([tokenizer.texts_to_sequences([in_text])[0]], maxlen=max_length)\n",
    "        # Predict the next word\n",
    "        yhat = np.argmax(model.predict([image.reshape(1, -1), sequence], verbose=0))\n",
    "        # Convert the predicted index to a word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        \n",
    "        if not word or word == 'endseq':  # Stop if word not found or end tag is reached\n",
    "            break\n",
    "        \n",
    "        in_text += f\" {word}\"  # Append the word to the input sequence\n",
    "    \n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bac3a6-e7a4-4662-91aa-07f798b36100",
   "metadata": {},
   "source": [
    "### 9. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28516e5-c3bc-4c59-8e58-dc4ce11405d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in tqdm(test):\n",
    "    # get actual caption\n",
    "    captions = mapping_dic[key]\n",
    "    # predict the caption for image\n",
    "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
    "    # split into words\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    # append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "# calcuate BLEU score\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb44eb-65d1-4190-9595-f027508e4536",
   "metadata": {},
   "source": [
    "## 10. Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a5a8e-197a-4b85-91b8-68a3adbddde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(img_path):\n",
    "    image = Image.open(img_path)\n",
    "    captions = mapping_dic[image_id]\n",
    "    print('---------------------Actual---------------------')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    # predict the caption\n",
    "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
    "    print('--------------------Predicted--------------------')\n",
    "    print(y_pred)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16bf9a8-8e38-4607-9076-ffe08320ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(\"../../resources/images_ibericam/albentosa-20230429-130501.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d6c04-97fc-436d-a34d-28abade040dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
