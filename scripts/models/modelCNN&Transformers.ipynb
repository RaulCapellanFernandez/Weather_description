{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e31c8f8-e113-4ca7-8948-c540ac4d6d22",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------\n",
    "## Base Model\n",
    "- Using CNN and RNN in a mixing way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "791acc2e-05da-435f-b5ec-dc69cd9fd2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.image import load_img , img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input\n",
    "from tensorflow.keras.applications import ResNet152\n",
    "from tensorflow.keras.applications.densenet import DenseNet201\n",
    "from tensorflow.keras.layers import Input , Dense , LSTM , Embedding , Dropout , add\n",
    "\n",
    "from tensorflow.keras.layers import Add, Activation, RepeatVector, Concatenate\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86432cab-9e3e-49fa-a9a5-ebc28e924efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_path = os.path.join('..', '..', 'resources')\n",
    "path_openAI    = os.path.join(resources_path, 'utils', 'images_weather_description_openAI.csv')\n",
    "path_ibericam  = os.path.join(resources_path, 'images_ibericam')\n",
    "path_captions  = os.path.join(resources_path, 'captions.txt')\n",
    "path_model     = os.path.join(resources_path, 'utils', 'best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9915dc-5763-413d-975e-6d55bfa5f0e4",
   "metadata": {},
   "source": [
    "### 1. Generate the mapping for captions in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e6c638-3a88-4f0e-9f89-bbd64da6117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_openAI   = pd.read_csv(path_openAI)\n",
    "df_openAI   = df_openAI.iloc[-301:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4618684-80c0-4301-852a-b75f26248ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux               = df_openAI[['path_image', 'description']]\n",
    "df_aux               = df_aux.copy()  # Create a full copy to avoid the warning\n",
    "df_aux['path_image'] = df_aux['path_image'].apply(lambda x: x.split(os.sep)[-1])\n",
    "df_aux.columns       = ['image', 'caption']\n",
    "\n",
    "# Get the name of the images\n",
    "df_aux['image_name'] = df_aux[\"image\"].str.split('.').str[0]\n",
    "\n",
    "# Cleaning and formating the dataset before applying LSTM\n",
    "df_aux['caption_mod'] = df_aux[\"caption\"].apply(lambda caption: [f'startseq \"{re.sub(\"[^A-Za-z . -]\", \"\", caption.lower())}\" endseq'])\n",
    "\n",
    "# Creating a dictinary with the name of the photos and the captions\n",
    "mapping_dic          = dict(zip(df_aux['image_name'], df_aux['caption_mod']))\n",
    "list_captions        = df_aux['caption_mod'].values\n",
    "list_captions        = [item[0] for item in list_captions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d804fc4-0ff2-4c94-abfa-7b527a4559bc",
   "metadata": {},
   "source": [
    "### 2. Load CNN prebuild models\n",
    "- Modify the preloaded models from keras to:\n",
    "    - Inputs: mantains the original inputs of the original model\n",
    "    - Outputs: Takes the second last layer of the model avoiding the last one to be used later as an input to the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df26e6bd-2eb1-4759-9f98-6e744a0591d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 16 layer network using 3x3 convolutions image classification #####\n",
    "model = VGG16()\n",
    "##### Dense networks with direct connections among all the layers to improve the gradient flow #####\n",
    "# model = DenseNet201()\n",
    "##### Residual networks, prevents performance degradation #####\n",
    "# model = ResNet152()\n",
    "\n",
    "model = Model(inputs = model.inputs , outputs = model.layers[-2].output)\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d4258-c79d-4e1b-ac6f-6427529f3e39",
   "metadata": {},
   "source": [
    "### 3. Transform the images\n",
    "- Open the images and adapt it to work with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "439f0b50-90e8-4897-8768-b0feb3d05599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ec4dff44264a31821c32541a17b9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = {}\n",
    "list_images_ibercam = glob(os.path.join(path_ibericam, '*.jpg'))\n",
    "\n",
    "for img_path in tqdm(list_images_ibercam):\n",
    "    # load the image from file in a certain format adapted to the model size\n",
    "    image = load_img(img_path, target_size=(224, 224))\n",
    "    \n",
    "    # convert image pixels to numpy array\n",
    "    image = img_to_array(image)\n",
    "    \n",
    "    # expand dimensions to add batch size dimension\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    # preprocess image for TensorFlow\n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    # extract features (model expects numpy array with batch dimension)\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    \n",
    "    # Get image name\n",
    "    image_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    \n",
    "    # Store feature\n",
    "    features[image_id] = feature.squeeze()  # Remove extra dimensions if present\n",
    "\n",
    "# Save features to pickle\n",
    "pickle.dump(features, open(os.path.join(resources_path, 'features.pkl'), 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc005e-e757-4b33-b7ca-0633785763e4",
   "metadata": {},
   "source": [
    "### 6. Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "018c41a1-2f07-4c03-82d5-c9a6ce992d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    # loop over images\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    \n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n += 1\n",
    "            captions = mapping[key]\n",
    "            # print(captions)\n",
    "            # process each caption\n",
    "            for caption in captions:\n",
    "                # encode the sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                # split the sequence into X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pairs\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\n",
    "                    # store the sequences\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n == batch_size:\n",
    "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                yield [X1, X2], y\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93fe8e-6ed1-471a-982a-aba7ac3fb6ec",
   "metadata": {},
   "source": [
    "### 4. Tokenizer the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd4cba5b-03bf-4e0b-8561-63c8297e0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Maximum size of one of the captions\n",
    "max_length = max(len(caption.split()) for caption in list_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1a61d-3e6e-44c2-84f8-b44e689b33a9",
   "metadata": {},
   "source": [
    "### 5. Create LSTM model\n",
    "- **shape=(4096,)** - output length of the features from the VGG model\n",
    "\n",
    "- **Dense** - single dimension linear layer array\n",
    "\n",
    "- **Dropout()** - used to add regularization to the data, avoiding over fitting & dropping out a fraction of the data from the layers\n",
    "\n",
    "- **model.compile()** - compilation of the model\n",
    "\n",
    "- **loss=’sparse_categorical_crossentropy’** - loss function for category outputs\n",
    "\n",
    "- **optimizer=’adam’** - automatically adjust the learning rate for the model over the no. of epochs\n",
    "\n",
    "- Model plot shows the concatenation of the inputs and outputs into a single layer\n",
    "\n",
    "- Feature extraction of image was already done using VGG, no CNN model was needed in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed0f708a-f952-4d82-ac97-73ea24118537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048,832</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ repeat_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ multi_head_attention          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,051,904</span> │ repeat_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)          │                           │                 │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
       "│                               │                           │                 │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
       "│                               │                           │                 │ repeat_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                               │                           │                 │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ layer_normalization           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285,000</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │       \u001b[38;5;34m1,048,832\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │       \u001b[38;5;34m1,280,000\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ repeat_vector (\u001b[38;5;33mRepeatVector\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ multi_head_attention          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │       \u001b[38;5;34m1,051,904\u001b[0m │ repeat_vector[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)          │                           │                 │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
       "│                               │                           │                 │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
       "│                               │                           │                 │ repeat_vector[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ multi_head_attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                               │                           │                 │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ layer_normalization           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │             \u001b[38;5;34m512\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                  │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │          \u001b[38;5;34m65,792\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m5000\u001b[0m)          │       \u001b[38;5;34m1,285,000\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,732,040</span> (18.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,732,040\u001b[0m (18.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,732,040</span> (18.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,732,040\u001b[0m (18.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters (you can adjust these)\n",
    "max_length = 50\n",
    "vocab_size = 5000\n",
    "embed_size = 256\n",
    "num_heads = 4  # number of attention heads\n",
    "\n",
    "# CNN Feature Input\n",
    "inputs1 = Input(shape=(4096,))  # Assuming you already have precomputed CNN features\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(embed_size, activation='relu')(fe1)  # Image embedding [batch_size, embed_size]\n",
    "\n",
    "# Expand image features for attention mechanism\n",
    "fe2_expanded = RepeatVector(max_length)(fe2)  # shape: (batch_size, max_length, embed_size)\n",
    "\n",
    "# Sequence Input\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, embed_size, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)  # shape: (batch_size, max_length, embed_size)\n",
    "\n",
    "# Attention Layer (Multi-head Attention)\n",
    "attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=embed_size)(query=se2, value=fe2_expanded, key=fe2_expanded)\n",
    "\n",
    "# Combine Attention Output with Embedding (residual connection)\n",
    "decoder_input = Add()([attention_out, se2])\n",
    "decoder_input = LayerNormalization()(decoder_input)\n",
    "\n",
    "# Feedforward Layer (can be expanded if desired)\n",
    "decoder_ff = Dense(embed_size, activation='relu')(decoder_input)\n",
    "\n",
    "# Generate caption output (per timestep prediction)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder_ff)\n",
    "\n",
    "# Final CNN-Attention Model\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443c99b-406c-4bbe-ae27-8ad7d4b2dfd3",
   "metadata": {},
   "source": [
    "### 5. Prepare data for trainning\n",
    "- Train split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65e21491-25d3-49f8-a077-52448e5e3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = list(mapping_dic.keys())\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe55387-ec74-4d5f-83e0-4ecb91e9b7aa",
   "metadata": {},
   "source": [
    "### 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8103ff38-4892-4c0d-bc2b-ab924f2dd16f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'el-pobo-20240821-203001'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m     generator = data_generator(train, mapping_dic, features, tokenizer, max_length, vocab_size, batch_size)\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# fit for one epoch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Save the best model\u001b[39;00m\n\u001b[32m     12\u001b[39m model.save(path_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tf_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mdata_generator\u001b[39m\u001b[34m(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size)\u001b[39m\n\u001b[32m     22\u001b[39m out_seq = to_categorical([out_seq],num_classes=vocab_size)[\u001b[32m0\u001b[39m]\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# store the sequences\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m X1.append(\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m])\n\u001b[32m     25\u001b[39m X2.append(in_seq)\n\u001b[32m     26\u001b[39m y.append(out_seq)\n",
      "\u001b[31mKeyError\u001b[39m: 'el-pobo-20240821-203001'"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "steps = len(train) // batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    # create data generator\n",
    "    generator = data_generator(train, mapping_dic, features, tokenizer, max_length, vocab_size, batch_size)\n",
    "    # fit for one epoch\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\n",
    "# Save the best model\n",
    "model.save(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f3c1e-be7c-4ac8-bc1a-23a3f45c1abb",
   "metadata": {},
   "source": [
    "### 8. Generate Captions for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "361a305e-1227-4c3b-9e8a-35f921c5b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "456395a4-8d4f-4101-9354-5f46cb91bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Encode and pad the input sequence\n",
    "        sequence = pad_sequences([tokenizer.texts_to_sequences([in_text])[0]], maxlen=max_length)\n",
    "        # Predict the next word\n",
    "        yhat = np.argmax(model.predict([image, sequence], verbose=0))\n",
    "        # Convert the predicted index to a word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        \n",
    "        if not word or word == 'endseq':  # Stop if word not found or end tag is reached\n",
    "            break\n",
    "        \n",
    "        in_text += f\" {word}\"  # Append the word to the input sequence\n",
    "    \n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bac3a6-e7a4-4662-91aa-07f798b36100",
   "metadata": {},
   "source": [
    "### 9. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d28516e5-c3bc-4c59-8e58-dc4ce11405d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bc95b8133344e79777f5ecff37b955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'el-pobo-20240905-063001'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m captions = mapping_dic[key]\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# predict the caption for image\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m y_pred = predict_caption(model, \u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m, tokenizer, max_length)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# split into words\u001b[39;00m\n\u001b[32m      9\u001b[39m actual_captions = [caption.split() \u001b[38;5;28;01mfor\u001b[39;00m caption \u001b[38;5;129;01min\u001b[39;00m captions]\n",
      "\u001b[31mKeyError\u001b[39m: 'el-pobo-20240905-063001'"
     ]
    }
   ],
   "source": [
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in tqdm(test):\n",
    "    # get actual caption\n",
    "    captions = mapping_dic[key]\n",
    "    # predict the caption for image\n",
    "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
    "    # split into words\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    # append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "# calcuate BLEU score\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb44eb-65d1-4190-9595-f027508e4536",
   "metadata": {},
   "source": [
    "## 10. Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a5a8e-197a-4b85-91b8-68a3adbddde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(img_path):\n",
    "    image = Image.open(img_path)\n",
    "    captions = mapping_dic[image_id]\n",
    "    print('---------------------Actual---------------------')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    # predict the caption\n",
    "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
    "    print('--------------------Predicted--------------------')\n",
    "    print(y_pred)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16bf9a8-8e38-4607-9076-ffe08320ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(\"../../resources/images_ibericam/el-pobo-20240822-073001.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
