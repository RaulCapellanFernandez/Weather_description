{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e31c8f8-e113-4ca7-8948-c540ac4d6d22",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------\n",
    "## Base Model\n",
    "- Using CNN and RNN in a mixing way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913338b2-cd50-4095-852e-ff64aec45347",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KerasLazyLoader' from 'tensorflow.python.util.lazy_loader' (C:\\Users\\Cp\\anaconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\__init__.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF2_BEHAVIOR\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'KerasLazyLoader' from 'tensorflow.python.util.lazy_loader' (C:\\Users\\Cp\\anaconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18be2e1e-e7e7-4e13-bbe0-0ed940a4d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras_hub.api.layers import TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791acc2e-05da-435f-b5ec-dc69cd9fd2d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KerasLazyLoader' from 'tensorflow.python.util.lazy_loader' (C:\\Users\\Cp\\anaconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleu_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corpus_bleu\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical, plot_model\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\__init__.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF2_BEHAVIOR\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'KerasLazyLoader' from 'tensorflow.python.util.lazy_loader' (C:\\Users\\Cp\\anaconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py)"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.image import load_img , img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input\n",
    "from tensorflow.keras.applications import ResNet152\n",
    "from tensorflow.keras.applications.densenet import DenseNet201\n",
    "from tensorflow.keras.layers import Input , Dense , LSTM , Embedding , Dropout , add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86432cab-9e3e-49fa-a9a5-ebc28e924efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_path = os.path.join('..', '..', 'resources')\n",
    "path_openAI    = os.path.join(resources_path, 'utils', 'images_weather_description_openAI.csv')\n",
    "path_ibericam  = os.path.join(resources_path, 'images_ibericam')\n",
    "path_captions  = os.path.join(resources_path, 'captions.txt')\n",
    "path_model     = os.path.join(resources_path, 'utils', 'best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9915dc-5763-413d-975e-6d55bfa5f0e4",
   "metadata": {},
   "source": [
    "### 1. Generate the mapping for captions in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6c638-3a88-4f0e-9f89-bbd64da6117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_openAI   = pd.read_csv(path_openAI)\n",
    "df_openAI   = df_openAI.iloc[-301:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4618684-80c0-4301-852a-b75f26248ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux               = df_openAI[['path_image', 'description']]\n",
    "df_aux               = df_aux.copy()  # Create a full copy to avoid the warning\n",
    "df_aux['path_image'] = df_aux['path_image'].apply(lambda x: x.split(os.sep)[-1])\n",
    "df_aux.columns       = ['image', 'caption']\n",
    "\n",
    "# Get the name of the images\n",
    "df_aux['image_name'] = df_aux[\"image\"].str.split('.').str[0]\n",
    "\n",
    "# Cleaning and formating the dataset before applying LSTM\n",
    "df_aux['caption_mod'] = df_aux[\"caption\"].apply(lambda caption: [f'startseq \"{re.sub(\"[^A-Za-z . -]\", \"\", caption.lower())}\" endseq'])\n",
    "\n",
    "# Creating a dictinary with the name of the photos and the captions\n",
    "mapping_dic          = dict(zip(df_aux['image_name'], df_aux['caption_mod']))\n",
    "list_captions        = df_aux['caption_mod'].values\n",
    "list_captions        = [item[0] for item in list_captions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d804fc4-0ff2-4c94-abfa-7b527a4559bc",
   "metadata": {},
   "source": [
    "### 2. Load CNN prebuild models\n",
    "- Modify the preloaded models from keras to:\n",
    "    - Inputs: mantains the original inputs of the original model\n",
    "    - Outputs: Takes the second last layer of the model avoiding the last one to be used later as an input to the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26e6bd-2eb1-4759-9f98-6e744a0591d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 16 layer network using 3x3 convolutions image classification #####\n",
    "model = VGG16()\n",
    "##### Dense networks with direct connections among all the layers to improve the gradient flow #####\n",
    "# model = DenseNet201()\n",
    "##### Residual networks, prevents performance degradation #####\n",
    "# model = ResNet152()\n",
    "\n",
    "model = Model(inputs = model.inputs , outputs = model.layers[-2].output)\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d4258-c79d-4e1b-ac6f-6427529f3e39",
   "metadata": {},
   "source": [
    "### 3. Transform the images\n",
    "- Open the images and adapt it to work with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f0b50-90e8-4897-8768-b0feb3d05599",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "list_images_ibercam = glob(os.path.join(path_ibericam, '*.jpg'))\n",
    "\n",
    "for index, img_path in enumerate(tqdm(list_images_ibercam)):\n",
    "    \n",
    "    # load the image from file in a certain format adapted to the model size\n",
    "    image = load_img(img_path, target_size= (224, 224))#(model.input.shape[1], model.input.shape[2]))\n",
    "    \n",
    "    # convert image pixels to numpy array\n",
    "    image = img_to_array(image)\n",
    "    \n",
    "    # reshape data for model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    \n",
    "    # preprocess image for tensorflow\n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    # extract features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    \n",
    "    # Get image name\n",
    "    image_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    \n",
    "    # store feature\n",
    "    features[image_id] = feature\n",
    "\n",
    "# Store the features of the images in a pickle file (for saving time for running multiple experiments)\n",
    "pickle.dump(features, open(os.path.join(resources_path, 'features.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc005e-e757-4b33-b7ca-0633785763e4",
   "metadata": {},
   "source": [
    "### 6. Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c41a1-2f07-4c03-82d5-c9a6ce992d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    # loop over images\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    \n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n += 1\n",
    "            captions = mapping[key]\n",
    "            # print(captions)\n",
    "            # process each caption\n",
    "            for caption in captions:\n",
    "                # encode the sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                # split the sequence into X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pairs\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\n",
    "                    # store the sequences\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n == batch_size:\n",
    "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                yield [X1, X2], y\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93fe8e-6ed1-471a-982a-aba7ac3fb6ec",
   "metadata": {},
   "source": [
    "### 4. Tokenizer the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4cba5b-03bf-4e0b-8561-63c8297e0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Maximum size of one of the captions\n",
    "max_length = max(len(caption.split()) for caption in list_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1a61d-3e6e-44c2-84f8-b44e689b33a9",
   "metadata": {},
   "source": [
    "### 5. Create LSTM model\n",
    "- **shape=(4096,)** - output length of the features from the VGG model\n",
    "\n",
    "- **Dense** - single dimension linear layer array\n",
    "\n",
    "- **Dropout()** - used to add regularization to the data, avoiding over fitting & dropping out a fraction of the data from the layers\n",
    "\n",
    "- **model.compile()** - compilation of the model\n",
    "\n",
    "- **loss=’sparse_categorical_crossentropy’** - loss function for category outputs\n",
    "\n",
    "- **optimizer=’adam’** - automatically adjust the learning rate for the model over the no. of epochs\n",
    "\n",
    "- Model plot shows the concatenation of the inputs and outputs into a single layer\n",
    "\n",
    "- Feature extraction of image was already done using VGG, no CNN model was needed in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f708a-f952-4d82-ac97-73ea24118537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder model\n",
    "# image feature layers\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "# sequence feature layers\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443c99b-406c-4bbe-ae27-8ad7d4b2dfd3",
   "metadata": {},
   "source": [
    "### 5. Prepare data for trainning\n",
    "- Train split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e21491-25d3-49f8-a077-52448e5e3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = list(mapping_dic.keys())\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe55387-ec74-4d5f-83e0-4ecb91e9b7aa",
   "metadata": {},
   "source": [
    "### 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103ff38-4892-4c0d-bc2b-ab924f2dd16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "steps = len(train) // batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    # create data generator\n",
    "    generator = data_generator(train, mapping_dic, features, tokenizer, max_length, vocab_size, batch_size)\n",
    "    # fit for one epoch\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\n",
    "# Save the best model\n",
    "model.save(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f3c1e-be7c-4ac8-bc1a-23a3f45c1abb",
   "metadata": {},
   "source": [
    "### 8. Generate Captions for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a305e-1227-4c3b-9e8a-35f921c5b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456395a4-8d4f-4101-9354-5f46cb91bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Encode and pad the input sequence\n",
    "        sequence = pad_sequences([tokenizer.texts_to_sequences([in_text])[0]], maxlen=max_length)\n",
    "        # Predict the next word\n",
    "        yhat = np.argmax(model.predict([image, sequence], verbose=0))\n",
    "        # Convert the predicted index to a word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        \n",
    "        if not word or word == 'endseq':  # Stop if word not found or end tag is reached\n",
    "            break\n",
    "        \n",
    "        in_text += f\" {word}\"  # Append the word to the input sequence\n",
    "    \n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bac3a6-e7a4-4662-91aa-07f798b36100",
   "metadata": {},
   "source": [
    "### 9. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28516e5-c3bc-4c59-8e58-dc4ce11405d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in tqdm(test):\n",
    "    # get actual caption\n",
    "    captions = mapping_dic[key]\n",
    "    # predict the caption for image\n",
    "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
    "    # split into words\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    # append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "# calcuate BLEU score\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb44eb-65d1-4190-9595-f027508e4536",
   "metadata": {},
   "source": [
    "## 10. Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a5a8e-197a-4b85-91b8-68a3adbddde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(img_path):\n",
    "    image = Image.open(img_path)\n",
    "    captions = mapping_dic[image_id]\n",
    "    print('---------------------Actual---------------------')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    # predict the caption\n",
    "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
    "    print('--------------------Predicted--------------------')\n",
    "    print(y_pred)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16bf9a8-8e38-4607-9076-ffe08320ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(\"../../resources/images_ibericam/el-pobo-20240822-073001.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
